# RoBERTa: A Robustly Optimized BERT Pretraining Approach

Ссылка на [статью](https://arxiv.org/pdf/1907.11692.pdf)

Статья на [habr](https://habr.com/ru/articles/680986/)

Модернизированный BERT

RoBERTa использует ту же архитектуру, что и BERT. Однако, в отличие от BERT, во время предобучения она обучается только генерации пропущенного токена (BERT также предобучался предсказанию следующего предложения). Ниже приведены некоторые изменения в гиперпараметрах, благодаря которым RoBERTa достигла производительности.

- Более длительное время обучения и больший объем обучающих данных (в 10 раз больше — от 16GB к 160GB)
- Размер батча от 256 к 8000 и больший словарь — от 30k to 50k
- В качестве входных данных используются более длинные последовательности, но RoBERTa по-прежнему имеет ограничение на максимальное количество токенов — 512, как и у BERT
- Динамическое маскирование позволяет маскирующей схеме меняться при каждой подаче последовательности на модель. Отличие от BERT в том, что везде используется одна и та же маскирующая схема.
- Убрали NSP
- Улучшили BPE-энкодинг, переведя базу с юникода на байты

## Данные
Как и в BERT используется Wikipedia + BookCorpus (16Гб). Но, помимо этого, они взяли новые датасеты - CC-News (63 миллиона новостей за 2.5 года - 76Гб), OpenWebText (корпус для GPT2 - скрауленые статьи, на которые присутствует как минимум 3 апвоута - 38Гб), Stories (корпус историй из CommonCrawl - 31Гб)

## Динамическое маскирование

Теперь маскирование не фиксировано, а выбирается динамически. Раньше выбиралось 10 масок и записывались таким образом в память (при 40 эпохах мы встречали 4 раза одно и то же).