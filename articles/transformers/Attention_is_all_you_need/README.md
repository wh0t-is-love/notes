# Attention is all you need

Ссылка на [статью](https://arxiv.org/pdf/1706.03762.pdf)

Понятно о архитектуре трансформера на [habr](https://habr.com/ru/articles/486358/)

- [Attention is all you need](#attention-is-all-you-need)
  - [Общий алгоритм](#общий-алгоритм)
  - [Что за матрицы Q, K, V и как их считать?](#что-за-матрицы-q-k-v-и-как-их-считать)
  - [Как считать Attention?](#как-считать-attention)
  - [Как считать Multi-Head Attention](#как-считать-multi-head-attention)
  - [Positional encoding](#positional-encoding)
  - [Cross-Attention](#cross-attention)
  - [Feed-Forward Network что и зачем?](#feed-forward-network-что-и-зачем)
  - [Маскирование в декодере](#маскирование-в-декодере)
  - [Layer normalization](#layer-normalization)

## Общий алгоритм

Трансформер - это архитектура encoder-decoder. На вход модели подается набор векторов, который получается путем токенизации исходного предложения.

Пусть мы решаем задачу машинного перевода, тогда общий алгоритм действий следующий:
- Текст преобразуется в вектор (путем токенизации)
- Вектор подается на вход энкодеру
- В энкодере вектор проходит несколько слоев, каждый из которых состоит из слоя Multi-Head Attention и Fully connected feed-forward network. Между ними присутствует слой LayerNorm, который применяется к конкатенации выхода из Multi-Head Attention и перед ним (также известный как residual connection).
- Начинаем работу с декодером
- Выход верхнего энкодера преобразуется в набор векторов внимания K и V. Они используются на каждом слое внимания в декодере.
- Выполняем $N$ раз, где $N$ = длине предложения (на самом деле нужно учесть специальные символы типа \<EOS\>,  \<BOS\>). На каждой итерации будем рассматривать следующее слово из предложения.
  - В декодер подаем исходное предложение на слой Masked Multi-Head Attention - специальный слой Attention, который позволяет модели видеть только текущее слово и те, которые он уже обработал.
  - Проходим через слои, схожие со слоями энкодера. Но K и V мы берем из векторов энкодера.


## Что за матрицы Q, K, V и как их считать?

- $Q$ - матрица запроса
- $K$ - матрица ключа
- $V$ - матрица значения

Получаются они путем умножения входного эмбеддинга $X$ на различные матрицы весов $W^Q$, $W^K$, $W^V$. Таким образом:

$$Q = X W^Q$$

$$K = X W^K$$

$$V = X W^V$$

Матрицы $W^Q$, $W^K$, $W^V$ являются матрицами весов, которые мы обучаем.

## Как считать Attention?

Пусть имеем $X \in \mathfrak{R}^{N\times d_{in}}$, из которого получили матрицы $Q$, $K$, $V$ $\in  \mathfrak{R}^{N\times d}$. Вообще говоря, матрицы не обязаны иметь одинаковую вторую размерность $d$, они могут быть и произвольными, главное только чтобы $V \in  \mathfrak{R}^{N\times d_{model}}$, а размерности $Q$ и $K$ совпадали ($d_K = d_Q$). Так как в основном используют одинаковый $d$, то для простоты будем брать его также одинаковым Конечная формула Attention запишется следующим образом:

$$Attention(Q, K, V) = \text{softmax}(\frac{QK^T}{\sqrt{d}})V$$

Разберемся по порядку:
- $QK^T \in \mathfrak{R}^{N\times N}$ - матрица, в которой $(QK^T)_{(i, j)}$ элемент является интерпретации важности влияния $j$ слова на $i$ слово. Для большей понятности рассмотрим следующее предложение: "The animal didn't cross the street because it was too tired". Как понять к чему относится "it" в этом предложении: к улице (street) или к животному (animal)? В данном случае это относится к "The animal" и соответсвующие и числа в этих ячейках будут больше чем в ячейках "the street".
- Умножение на $\frac{1}{\sqrt{d}}$. Каждую ячейку мы умножаем на данное значение. Делается это с целью сделать дальнейшее применение softmax более стабильным.
- Softmax. $\text{Softmax} (x_i) = \frac{e^{x_i}}{\sum{e^{x_j}}}$. Мы нормализуем данные построчно.
- Умножаем полученную матрицу на $V$. Мы хотим обращать внимание только на интересные слова, поэтому при умножении слова с маленькими значениями практически не внесут никаких изменений в конечный результат.
  
## Как считать Multi-Head Attention

Теперь мы имеем не просто матрицы $W^Q$, $W^K$, $W^V$ $\in \mathfrak{R}^{d_{in}\times d}$, а $W^Q_i$, $W^K_i$, $W^V_i$ $\in \mathfrak{R}^{d_{in}\times d_h}$, где $d_h = \frac{d}{h}$, $i \in [1, h]$. Обычно $h$ берут кратным $d$, но это не обязательно. В таком случае $d_h = \lfloor \frac{d}{h}\rfloor$.

Умножая $X$ на $W^Q_i$, $W^K_i$, $W^V_i$, мы получаем матрицы $Q_i$, $K_i$, $V_i$. Для каждого набора мы считаем Attention (полученные значения принято называть "головами"), конкатенируем полученные головы и умножаем на еще одну матрицу $W^O \in \mathfrak{R}^{hd_h\times d}$. Общая формула получается следующая:

$$\text{MultiHead}(Q, K, V) = \text{Concat}(head_1, \dots, head_h)W^O$$

$$head_i = \text{Attention}(Q_i, K_i, V_i)$$

## Positional encoding

Способ модели учесть порядок слов в тексте. Для каждой ячейки мы прибавляем некоторое значение, которое позволяет легко вычислить расстояние между конкретными словами. Считать данные прибавки можно, например, так:

$$PE(pos,2i) =sin(pos/10000^{2i/d_{model}})$$

$$PE(pos,2i+1) =cos(pos/10000^{2i/d_{model}})$$

Важно понимать, что расстояние в данном случае считается не с точки зрения абсолютного сравнения, а с точки зрения скалярного произведения.

## Cross-Attention

В декодере используется не совсем классический механизм Attention, какой был в энкодере. Матрица $Q$ получается из вектора предыдущего слоя декодера, а $K$ и $V$ из выхода энкодера. То есть, если $S_e$, $S_d$ - выходы энкодера и предыдущего слоя декодера, то:

$$Q = S_dW^Q$$

$$K = S_eW^K$$

$$V = S_eW^V$$

## Feed-Forward Network что и зачем?

Feed-Forward Network - матрица прямого распространения. Обычная линейная сетка.

В трансформерах FFN стоит после слоя Attention и, как правило, имеет один скрытый слой. Размер скрытого слоя обычно в 4 раза больше чем скрытое пространство. Размерность выходного слоя равна размерности входного. Как правило в качестве функции активации используется ReLU. Таким образом, данную сеть можно представить в виде формулы:

$$FFN(x) = max(0, xW_1 + b_1 )W_2 + b_2$$

где $W_i$, $b_i$ - обучаемые веса

Стоит упомянуть, что на вход сетке подается не выходная матрица, а каждая ее строчка отдельно, что расширяет горизонты для распараллеливания вычислений.

## Маскирование в декодере

Мы не хотим, чтобы при декодировании мы "заглядывали в будущее", смотря на следующие слова в предложении после декодируемого.

В слое Masked Multi-Head Attention декодера перед тем как посчитать softmax, мы умножаем полученную на предыдущих шагах матрицу на вектор $[1, 1, \dots, 1, -\infty, -\infty, \dots, -\infty] \in (\mathfrak{R} \cup \{-\infty\})^{N}$. Количество единиц равно текущему шага алгоритма декодирования. Таким образом, при применении softmax все веса слов после текущего обнулятся.

## Layer normalization

Мы много говорили про layer normalization, но не разбирали, что это такое. Пришло время сделать и это.

Формула у данной нормализации следующая:

$$LN(x) = \gamma\frac{x - \bar{x}}{\sigma} + \beta$$

где

$$\bar{x} = \frac{1}{d}\sum_{i=1}^dx_i$$

$$\sigma^2 = \frac{1}{d}\sum_{i=1}^d(x_i - \bar{x})^2$$

$$\gamma\text{, }\beta\text{ - обучаемые параметры}$$

Мы проводим нормализацию по каждой строчке полученной матрицы в трансформере.
